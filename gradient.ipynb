{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. What is Gradient Boosting Regression?\n",
    "\"\"\"\n",
    "  Gradient Boosting regression is a type of machine learning algorithm is used for regression tasks ,which involve \n",
    "  predicting a continious numerical output based on a set of input features .\n",
    "\n",
    "GBR works by iteratively adding decision trees to the ensemble, with each new tree trained to correct the errors of the \n",
    "previous trees in the ensemble. GBR builds a sequence of regression models, where each model is trained to\n",
    " predict the residual errors of the previous model. The final prediction is obtained by adding up the predictions of all \n",
    " the models in the sequence.\n",
    "\n",
    "The gradient in GBR refers to the use of gradient descent optimization to minimize the loss function, which measures the \n",
    "difference between the predicted and actual values. This involves finding the optimal weights for each tree, so that the \n",
    "ensemble minimizes the loss function on the training data.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 1.262823920870276e-09\n",
      "R-squared: 0.9999999989476467\n"
     ]
    }
   ],
   "source": [
    "# Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "# simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "# performance using metrics such as mean squared error and R-squared.\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "        self.weights = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.mean = np.mean(y)\n",
    "        y_pred = np.full_like(y, self.mean, dtype=np.float64)\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            residuals = y - y_pred\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "            self.weights.append(self.learning_rate)\n",
    "            y_pred += self.learning_rate * tree.predict(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = np.full(X.shape[0], self.mean, dtype=np.float64)\n",
    "\n",
    "        for i, tree in enumerate(self.trees):\n",
    "            y_pred += self.weights[i] * tree.predict(X)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "# Sample data for regression\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([2, 4, 5, 4, 5], dtype=np.float64)\n",
    "\n",
    "# Train the model\n",
    "model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X)\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "# optimise the performance of the model. Use grid search or random search to find the best\n",
    "# hyperparameters\n",
    "\n",
    "\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "        self.weights = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.mean = np.mean(y)\n",
    "        y_pred = np.full_like(y, self.mean)\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            residuals = y - y_pred\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "            self.weights.append(self.learning_rate)\n",
    "            y_pred += self.learning_rate * tree.predict(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = np.full(X.shape[0], self.mean)\n",
    "\n",
    "        for i, tree in enumerate(self.trees):\n",
    "            y_pred += self.weights[i] * tree.predict(X)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        return r2_score(y, y_pred)\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "# Sample data for regression\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([2, 4, 5, 4, 5])\n",
    "\n",
    "# Create a GradientBoostingRegressor object\n",
    "model = GradientBoostingRegressor()\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.5],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV object to search for the best hyperparameters\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5)\n",
    "\n",
    "# Fit the GridSearchCV object to the data\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the best hyperparameters and score\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. What is a weak learner in Gradient Boosting?\n",
    "\"\"\"In Gradient Boosting, a weak learner is a simple model that is not capable of making accurate predictions on its own\n",
    " but can contribute to the overall accuracy when combined with other weak learners. In the context of decision trees, \n",
    " a weak learner can be a tree with low depth or limited number of leaf nodes. The idea behind using weak learners is\n",
    "  to combine their predictions in an additive manner to obtain a strong learner that can achieve high accuracy.\n",
    "   The boosting algorithm trains the weak learners sequentially, with each tree trying to correct the errors of the\n",
    "    previous tree. The final prediction is the weighted sum of the predictions of all weak learners.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "\"\"\"The intuition behind the Gradient Boosting algorithm is to create a powerful ensemble model by combining several weak learners.\n",
    " The algorithm works by sequentially training a series of weak learners, such as decision trees, and adding them to the ensemble \n",
    " model. Each tree tries to correct the errors of the previous tree by fitting to the residual errors of the current predictions.\n",
    "  This process is repeated multiple times until the ensemble model can no longer improve.\n",
    "\n",
    "The name \"Gradient Boosting\" comes from the fact that the algorithm uses gradient descent optimization to minimize the loss \n",
    "function of the ensemble model. The gradient of the loss function is computed with respect to the predictions of the current\n",
    " ensemble model and is used to update the weights of the weak learners. The updates are made in such a way that each subsequent\n",
    "  weak learner focuses on the errors made by the previous learners, thereby reducing the overall error of the ensemble model.\n",
    "\n",
    "The intuition behind Gradient Boosting is that by combining several weak learners, we can create a more powerful model that\n",
    " can generalize well to new data. The algorithm is particularly effective in handling complex and non-linear relationships \n",
    " between the input and output variables. However, it can be sensitive to overfitting, so careful tuning of the hyperparameters\n",
    "  and regularization techniques are necessary to achieve optimal performance.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "\"\"\"The Gradient Boosting algorithm builds an ensemble of weak learners in a sequential manner. At each iteration, it fits a new\n",
    " model to the negative gradient of the loss function with respect to the predicted output of the previous model. The new model\n",
    "  is then added to the ensemble, and its prediction is combined with the predictions of the previous models to make a new\n",
    "   prediction. The process is repeated iteratively, with each new model attempting to correct the errors of the previous models.\n",
    "    The final prediction is a weighted sum of the predictions of all the models in the ensemble, where the weights are determined \n",
    "    by the learning rate and the performance of the individual models. By combining the predictions of many weak learners, \n",
    "    the Gradient Boosting algorithm can build a strong learner that can make accurate predictions on a variety of problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
    "# algorithm?\n",
    "\"\"\"The mathematical intuition behind the Gradient Boosting algorithm involves the following steps:\n",
    "\n",
    "(1)Define a loss function--- The first step is to define a loss function that measures the difference between the predicted values\n",
    " and the actual values of the target variable. The loss function should be differentiable so that the gradient can be computed.\n",
    "\n",
    "(2)Fit a model to the data--- The second step is to fit a model to the data using the loss function as the objective function to \n",
    "minimize. The model can be a simple model such as a decision tree or a more complex model such as a neural network.\n",
    "\n",
    "(3)Compute the residuals--- Once the model is trained, the residuals are computed by subtracting the predicted values from the\n",
    " actual values of the target variable.\n",
    "\n",
    "(4)Fit a new model to the residuals--- The next step is to fit a new model to the residuals. This model should be a weak learner, \n",
    "i.e., a model that is only slightly better than random guessing.\n",
    "\n",
    "(5)Add the new model to the ensemble--- Once the new model is trained, it is added to the ensemble of models. The ensemble is updated\n",
    " by adding the weighted sum of the predictions of all the models in the ensemble.\n",
    "\n",
    "(6)Repeat steps 3 to 5: Steps 3 to 5 are repeated until a stopping criterion is met. The stopping criterion can be a maximum number\n",
    " of iterations or a minimum improvement in the loss function.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
